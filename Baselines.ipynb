import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression

from sklearn.ensemble import RandomForestClassifier

from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix



# 1. Load Data

df = pd.read_csv('dataset.csv')



# 2. Data Cleaning: Missing Values

df = df.dropna()



# 3. Data Cleaning: Outlier Handling

# Capping numerical columns at 99th percentile

num_cols = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price']

for col in num_cols:

    upper_limit = df[col].quantile(0.99)

    df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])



# 4. Feature Selection (Recalculating after outlier handling)

X = df.drop('fraud', axis=1)

y = df['fraud']



rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)

rf_temp.fit(X, y)

importances = rf_temp.feature_importances_

top_indices = np.argsort(importances)[-4:]

top_features = X.columns[top_indices].tolist()



print(f"Selected Top 4 Features: {top_features}")



# 5. Visualizations (EDA)

# Correlation Heatmap

plt.figure(figsize=(10, 8))

sns.heatmap(df[top_features + ['fraud']].corr(), annot=True, cmap='coolwarm', fmt='.2f')

plt.title('Correlation Heatmap of Top Features and Fraud')

plt.savefig('correlation_heatmap.png')

plt.close()



# Pairplot (Sampling 1000 points for speed)

sample_df = df.sample(1000, random_state=42)

sns.pairplot(sample_df[top_features + ['fraud']], hue='fraud', diag_kind='kde')

plt.suptitle('Pairplot of Top 4 Features', y=1.02)

plt.savefig('pairplot.png')

plt.close()



# 6. Prepare data for modeling

X_reduced = df[top_features]

X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42, stratify=y)



scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)



# 7. Train Classical Baselines

models = {

    "Logistic Regression": LogisticRegression(),

    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),

    "Small Neural Network (MLP)": MLPClassifier(hidden_layer_sizes=(8, 4), max_iter=500, random_state=42)

}



results = []

for name, model in models.items():

    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)

    y_prob = model.predict_proba(X_test_scaled)[:, 1]

    

    acc = accuracy_score(y_test, y_pred)

    auc = roc_auc_score(y_test, y_prob)

    results.append({'Model': name, 'Accuracy': acc, 'AUC-ROC': auc})



results_df = pd.DataFrame(results)

print("\nClassical Baseline Performance:")

print(results_df)



# 8. Save final cleaned dataset for next steps

df_final = pd.concat([X_reduced, y], axis=1)

df_final.to_csv('final_processed_data.csv', index=False)

print("\nFinal processed data saved to 'final_processed_data.csv'")
